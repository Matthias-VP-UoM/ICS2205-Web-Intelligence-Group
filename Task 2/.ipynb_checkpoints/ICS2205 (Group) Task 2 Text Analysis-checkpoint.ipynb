{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5567ec9",
   "metadata": {},
   "source": [
    "# Part 1 - Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df866174",
   "metadata": {},
   "source": [
    "## a) Headline Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f40a5",
   "metadata": {},
   "source": [
    "### i) Text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c3e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "\n",
    "# relative file path to dataset\n",
    "data = 'data/NewsCategoryDataset_2017_2022.json'\n",
    "parsedList = []\n",
    "categories = {}\n",
    "documentDetailList = []\n",
    "\n",
    "# iteratively parse and append each file to parsedList\n",
    "with open(data, 'r') as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)\n",
    "        headline = record.get(\"headline\", \"\")\n",
    "        \n",
    "        shortDescription = record.get(\"short_description\", \"\")\n",
    "        parsedList.append(headline + \" \" + shortDescription)\n",
    "        category = record.get(\"category\", \"\")\n",
    "        \n",
    "        documentDetails = {\n",
    "            'link': record.get(\"link\"),\n",
    "            'headline': headline,\n",
    "            'category': category,\n",
    "            'description': shortDescription,\n",
    "            'authors': record.get(\"authors\"),\n",
    "            'date': record.get(\"date\")\n",
    "        }\n",
    "        documentDetailList.append(documentDetails)\n",
    "\n",
    "        # Update categories dictionary\n",
    "        if category not in categories:\n",
    "            categories[category] = [len(parsedList)-1]\n",
    "        else:\n",
    "            categories[category].append(len(parsedList)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864419d0",
   "metadata": {},
   "source": [
    "### ii) Lexical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f9d215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenList = [nltk.tokenize.word_tokenize(d) for d in parsedList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e904de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaList = [[t for t in d if t.isalpha()] for d in tokenList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff408c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowerList = [[t.lower() for t in d] for d in alphaList]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16bdd20",
   "metadata": {},
   "source": [
    "### iii) Filtering stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beab72cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "filtList = [[t for t in d if (not t in stopwords)] for d in lowerList]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305ac0b",
   "metadata": {},
   "source": [
    "### iv) Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d96397",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "finalList = [[stemmer.stem(t) for t in d] for d in filtList]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49109db2",
   "metadata": {},
   "source": [
    "## b) TF.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ec1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "def calculateTF(t, d):\n",
    "    # Calculate Term Frequency (TF)\n",
    "    # TF = number of occurrences / number of terms in d\n",
    "    return d.count(t) / len(d)\n",
    "\n",
    "def calculateIDF(t, inverted_index, total_documents):\n",
    "    # Calculate Inverse Document Frequency (IDF)\n",
    "    # IDF = log(1 + N / 1 + df)\n",
    "    # Slight modification to account for terms which never appear, leading to a divide-by-zero error\n",
    "    \n",
    "    # Number of documents containing term (N)\n",
    "    relevantDocCount = len(inverted_index[t])\n",
    "\n",
    "    return math.log(1 + total_documents / (1 + relevantDocCount))\n",
    "\n",
    "def generateInvertedIndex(corpus):\n",
    "    # Create an inverted index\n",
    "    invertedIndex = defaultdict(list)\n",
    "    \n",
    "    # Populate the inverted index\n",
    "    for i, document in enumerate(corpus):\n",
    "        for term in set(document):\n",
    "            invertedIndex[term].append(i)\n",
    "    \n",
    "    return invertedIndex\n",
    "\n",
    "def generateTDM(corpus, invertedIndex):\n",
    "    # Extract unique terms\n",
    "    unique = list(invertedIndex.keys())\n",
    "\n",
    "    # Initialize a sparse Term-Document matrix\n",
    "    tdm_tfidf = dok_matrix((len(corpus), len(unique)), dtype=np.float64)\n",
    "\n",
    "    # Populate the sparse matrix\n",
    "    for doc_id, document in enumerate(corpus):\n",
    "        for term_id, term in enumerate(unique):\n",
    "            tf = calculateTF(term, document)\n",
    "            idf = calculateIDF(term, invertedIndex, len(corpus))\n",
    "            tfidf = tf * idf\n",
    "            tdm_tfidf[doc_id, term_id] = tfidf\n",
    "\n",
    "    return tdm_tfidf, unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f8af1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "invertedIndex = generateInvertedIndex(finalList)\n",
    "matrix, terms = generateTDM(finalList, invertedIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c5001",
   "metadata": {},
   "source": [
    "## c) Extract highest n% for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a9c80",
   "metadata": {},
   "source": [
    "### i) Average term weights for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0055d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculateAverageCategoryTermWeight(tdm_tfidf, categories, unique):\n",
    "    avgTermWeightDict = {}\n",
    "    \n",
    "    for category, indexes in categories.items():\n",
    "        # list to store average term weights for each term in the category\n",
    "        avgTermWeightVector = []\n",
    "\n",
    "        # extract relevant rows from the TF.IDF matrix for the current category\n",
    "        categoryMatrix = tdm_tfidf[indexes].todense()\n",
    "        # transpose the matrix to get term-wise data\n",
    "        categoryMatrix = np.array(categoryMatrix).T\n",
    "\n",
    "        for i, term in enumerate(unique):\n",
    "            # extract term weights for the current term\n",
    "            termWeights = categoryMatrix[i]\n",
    "\n",
    "            avgTermWeight = np.mean(termWeights)\n",
    "            avgTermWeightVector.append(avgTermWeight)\n",
    "\n",
    "        # store the average term weights for the current category\n",
    "        avgTermWeightDict[category] = avgTermWeightVector\n",
    "\n",
    "    return avgTermWeightDict\n",
    "\n",
    "avgCategoryTermWeights = calculateAverageCategoryTermWeight(matrix, categories, terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977df5c7",
   "metadata": {},
   "source": [
    "### ii) Get highest weighted n% terms per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "164b19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopNTerms(avgTermWeights, unique, n):\n",
    "    topTermsDict = {}\n",
    "\n",
    "    for category, termWeightVector in avgTermWeights.items():\n",
    "        # calculate the threshold weight for the top n% terms\n",
    "        threshold = np.percentile(termWeightVector, 100 - n)\n",
    "        #print(f\"Category: {category}, Threshold: {threshold}\")\n",
    "        \n",
    "        if (threshold == 0.0):\n",
    "            # get the indices of terms that exceed the median weight\n",
    "            topIndices = [i for i, weight in enumerate(termWeightVector) if weight > 0.0]\n",
    "        else:\n",
    "            # get the indices of terms that exceed the threshold weight\n",
    "            topIndices = [i for i, weight in enumerate(termWeightVector) if weight > threshold]\n",
    "\n",
    "        topTerms = [(unique[i], termWeightVector[i]) for i in topIndices]\n",
    "\n",
    "        topTermsDict[category] = topTerms\n",
    "\n",
    "    return topTermsDict\n",
    "\n",
    "topCategoryTerms = getTopNTerms(avgCategoryTermWeights, terms, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8c6d18",
   "metadata": {},
   "source": [
    "### iii) Export category details to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8125a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def exportCategoryJSON(categories, topTerms, path):\n",
    "    categoryDetails = []\n",
    "\n",
    "    for category in categories:\n",
    "        # create a dictionary to store category details\n",
    "        categoryDetail = {\n",
    "            'category_name': category,\n",
    "            'documents': categories[category],\n",
    "            'top_terms': topTerms[category]\n",
    "        }\n",
    "\n",
    "        # add the category details to the list\n",
    "        categoryDetails.append(categoryDetail)\n",
    "\n",
    "    # export category details as JSON\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(categoryDetails, file, indent=2)\n",
    "\n",
    "outputPath = 'data/category_details.json'\n",
    "exportCategoryJSON(categories, topCategoryTerms, outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eba0353",
   "metadata": {},
   "source": [
    "## d) k-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2812b3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/30\n",
      "2/30\n",
      "3/30\n",
      "4/30\n",
      "5/30\n",
      "6/30\n",
      "7/30\n",
      "8/30\n",
      "9/30\n",
      "10/30\n",
      "11/30\n",
      "12/30\n",
      "13/30\n",
      "14/30\n",
      "15/30\n",
      "16/30\n",
      "17/30\n",
      "18/30\n",
      "19/30\n",
      "20/30\n",
      "21/30\n",
      "22/30\n",
      "23/30\n",
      "24/30\n",
      "25/30\n",
      "26/30\n",
      "27/30\n",
      "28/30\n",
      "29/30\n",
      "30/30\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def dotProduct(v1, v2):\n",
    "    return v1.multiply(v2).sum()\n",
    "\n",
    "def sumOfSquares(v):\n",
    "    return np.sum(v.power(2))\n",
    "\n",
    "def cosine_similarity(query, document, querySquared, docSquared):\n",
    "    dp = dotProduct(query, document)\n",
    "\n",
    "    if querySquared == 0 or docSquared == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "\n",
    "    return dp / math.sqrt(querySquared * docSquared)\n",
    "\n",
    "def precomputeSumOfSquares(data):\n",
    "    return [sumOfSquares(vector) for vector in data]\n",
    "\n",
    "def assignClusters(data, centroids, sumOfSqauresVector):\n",
    "    clusters = {i: [] for i in range(len(centroids))}\n",
    "    \n",
    "    for i, vector in enumerate(data):\n",
    "        maxSimilarity = float('-inf')\n",
    "        currentCluster = -1\n",
    "\n",
    "        for j, centroid in enumerate(centroids):\n",
    "            similarity = cosine_similarity(vector, centroid, sumOfSqauresVector[i], sumOfSqauresVector[j])\n",
    "            if similarity > maxSimilarity:\n",
    "                maxSimilarity = similarity\n",
    "                currentCluster = j\n",
    "\n",
    "        clusters[currentCluster].append(i)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def updateCentroids(data, clusters):\n",
    "    centroids = []\n",
    "\n",
    "    for indices in clusters.values():\n",
    "        clusterVectors = [data[i] for i in indices]\n",
    "        centroid = np.sum(clusterVectors, axis=0) / len(clusterVectors)\n",
    "        centroids.append(centroid.toarray().flatten())\n",
    "\n",
    "    return centroids\n",
    "\n",
    "def k_means_clustering(data, k, maxIterations=30):\n",
    "    # initialize centroids randomly\n",
    "    centroids = [data[i] for i in random.sample(range(data.shape[0]), k)]\n",
    "\n",
    "    # precompute squared magnitudes for data vectors\n",
    "    sumOfSqauresVector = precomputeSumOfSquares(data)\n",
    "\n",
    "    for i in range(maxIterations):\n",
    "        # assign vectors to clusters\n",
    "        clusters = assignClusters(data, centroids, sumOfSqauresVector)\n",
    "\n",
    "        newCentroids = updateCentroids(data, clusters)\n",
    "\n",
    "        # check for convergence\n",
    "        if np.array_equal(newCentroids, centroids):\n",
    "            break\n",
    "\n",
    "        centroids = newCentroids\n",
    "        \n",
    "        #print(f\"{i+1}/{maxIterations}\")\n",
    "\n",
    "    return clusters\n",
    "\n",
    "k = 5\n",
    "clusters = k_means_clustering(matrix, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f32fa",
   "metadata": {},
   "source": [
    "## e) Extract highest n% for each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af49780",
   "metadata": {},
   "source": [
    "### i) Average term weights for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a623515",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculateAverageClusterTermWeight(tdm_tfidf, clusters, unique):\n",
    "    avgTermWeightDict = {}\n",
    "\n",
    "    for cluster, indexes in clusters.items():\n",
    "        # list to store average term weights for each term in the cluster\n",
    "        avgTermWeightVector = []\n",
    "\n",
    "        # extract relevant rows from the TF.IDF matrix for the current cluster\n",
    "        clusterMatrix = dok_matrix((len(indexes), tdm_tfidf.shape[1]), dtype=tdm_tfidf.dtype)\n",
    "        for i, index in enumerate(indexes):\n",
    "            clusterMatrix[i, :] = tdm_tfidf[index, :]\n",
    "            \n",
    "        # transpose the matrix to get term-wise data\n",
    "        clusterMatrix = clusterMatrix.transpose()\n",
    "\n",
    "        for i, term in enumerate(unique):\n",
    "            # extract term weights for the current term\n",
    "            termWeights = clusterMatrix.getrow(i).toarray()[0]\n",
    "\n",
    "            avgTermWeight = np.mean(termWeights)\n",
    "            avgTermWeightVector.append(avgTermWeight)\n",
    "\n",
    "        # store the average term weights for the current cluster\n",
    "        avgTermWeightDict[cluster] = avgTermWeightVector\n",
    "\n",
    "    return avgTermWeightDict\n",
    "\n",
    "avgClusterTermWeights = calculateAverageClusterTermWeight(matrix, clusters, terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5f189",
   "metadata": {},
   "source": [
    "### ii) Get highest n% terms per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3d9dc7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topClusterTerms = getTopNTerms(avgClusterTermWeights, terms, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4922e56a",
   "metadata": {},
   "source": [
    "### iii) Export cluster details to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8c82946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def exportClusterJSON(clusters, topTerms, path):\n",
    "    clusterDetails = []\n",
    "\n",
    "    for cluster in clusters:\n",
    "        # create a dictionary to store cluster details\n",
    "        clusterDetail = {\n",
    "            'cluster_index': cluster,\n",
    "            'documents': clusters[cluster],\n",
    "            'top_terms': topTerms[cluster]\n",
    "        }\n",
    "\n",
    "        # add the cluster details to the list\n",
    "        clusterDetails.append(clusterDetail)\n",
    "\n",
    "    # export cluster details as JSON\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(clusterDetails, file, indent=2)\n",
    "\n",
    "outputPath = 'data/cluster_details.json'\n",
    "exportClusterJSON(clusters, topClusterTerms, outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41818eb",
   "metadata": {},
   "source": [
    "# \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19fe17",
   "metadata": {},
   "source": [
    "# Part 2 - Web application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a636099",
   "metadata": {},
   "source": [
    "## Extract and export top weighted n% headline terms to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f91aac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def exportTopNPercentTermsJSON(tdm_tfidf, documentList, unique, path, n):\n",
    "    for i, documentDetails in enumerate(documentList):\n",
    "        documentVector = tdm_tfidf.getrow(i).toarray()[0]\n",
    "        \n",
    "        threshold = np.percentile(documentVector, 100 - n)\n",
    "        \n",
    "        if (threshold == 0.0):\n",
    "            # get the indices of terms that exceed the median weight\n",
    "            topIndices = [i for i, weight in enumerate(documentVector) if weight > 0.0]\n",
    "        else:\n",
    "            # get the indices of terms that exceed the threshold weight\n",
    "            topIndices = [i for i, weight in enumerate(documentVector) if weight > threshold]\n",
    "        \n",
    "        # get the corresponding terms and their weight\n",
    "        topTerms = [(unique[i], documentVector[i]) for i in topIndices]\n",
    "\n",
    "        documentDetails['top_n_percent_terms'] = topTerms\n",
    "\n",
    "    # export document details as JSON\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(documentList, file, indent=2)\n",
    "\n",
    "outputPath = 'data/headlines.json'\n",
    "exportTopNPercentTermsJSON(matrix, documentDetailList, terms, outputPath, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a078a08a",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65350d",
   "metadata": {},
   "source": [
    "To start the Flask application simply run 'python /path/to/folder/app.py' in your terminal. The data/ directory contains pre-computed JSON files necessary for the web application. If you wish to compute them again with different parameters, run the appropriate cells with the updated parameters and they will write to the data/ directory. \n",
    "\n",
    "For the wordcloud charts, d3 along with the d3-cloud plugin was used to create the wordcloud. Due to the varying weights for each grouping (categories/clusters), finding a single constant to multiply to find an appropriate font size required the use of a simple function which checks the weight of a single term. Reference used: https://d3-graph-gallery.com/wordcloud.html.\n",
    "\n",
    "For the bubble charts, https://webtips.dev/how-to-make-interactive-bubble-charts-in-d3-js was used as a reference during development. Nodes under a certain size tended to obscure the text, so a mouseover function was used to show the name of the node on hover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78734dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
